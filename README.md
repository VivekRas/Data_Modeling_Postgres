# Data Modelling and building an ETL pipeline
## Tools - Python & Postgres

Defined fact and dimension tables using star schema for particular analytics focus, and built an ETL pipeline that transfers data from files in two local directories into respective tables in Postgres using Python and SQL.

## Purpose of this database 
Sparkify wants to know what their users are listening to. The current format of the data (json files)
 as songs and log files, does not enable them to query, analyse and draw meaningfull 
insights from it

Their purpose is three fold 
1. They need data in a format that can help them effectively analyse data 
2. They need a Database model which enables them to run simple queries, that take the minimum amount of time while giving them wholesome data
3. They need a data pipeline to be created, that can automatically update the data base

## Project Template

1. `create_tables.py` drops and creates new tables. We run this file to reset your tables each time, before running our ETL scripts.
2. `etl.py` reads and processes files from song_data and log_data and loads them into our tables. 
3. `sql_queries.py` contains all your sql queries, and is imported into the files above.

## Database schema design

To meet the requirements, implemented a star schema  
1. This is a small DB 
2. We want to enable the user to quickly get the data without writing complex joins. 
3. Since it allows user ease of access, with only 2NF level normalisation

## Understanding the Input data

**Song Dataset**

This dataset is a subset of real data from the “Million Song Dataset”. Each file is in JSON format and contains metadata about a song and the artist of that song.

**Log Dataset**

This dataset consists of log files in JSON format generated by an event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.


## Schema for Song Play Analysis
Using the song and log datasets, we have created a star schema optimized for queries on song play analysis. This includes the following tables.

**Fact Table**
1. songplays - records in log data associated with song plays i.e. records with page NextSong

**Dimension Tables**
2. users - users in the app
3. songs - songs in music database
4. artists - artists in music database
5. time - timestamps of records in songplays broken down into specific units

_Data is in a structured form (extracted from json format to a more readable table format)_
_We have done data quilty checks - Cleaned it up, removed duplicated, removed null values_

## ELT pipeline (Steps to run the program)

0. Make sure that the data is stored in the filepath='data/song_data' for songs data and filepath='data/log_data' for logs data
1. cmd > "python create_tables.py" > enter
2. cmd > "python etl.py" > enter

(The respective Postgres database tables gets updated with the records)
